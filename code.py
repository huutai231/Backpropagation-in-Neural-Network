# -*- coding: utf-8 -*-
"""21119375.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KscUjzSP4Fvx9V7UAr4SOHkGeAfDQFWj
"""

import numpy as np
import tensorflow as tf

# Load dataset
print("LOAD MNIST DATABASE")
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
#print(x_train.shape)     #  output: (60000, 28, 28)  =>>  60000 bức ảnh kích thước 28x28
#print(x_train[0])        #  output: Xem các giá trị trong 1 matrix từ 0->255

# Đưa mỗi data về dạng 1 vector với 28x28=784 giá trị thực từ 0->1 bằng cách chia 255.0
x_train = np.reshape(x_train, (60000, 784))/255.0
x_test = np.reshape(x_test, (10000, 784))/255.0

# Label ở dạng số nguyên ---> chuyển về 1 vector 10 phần tử dạng one hot
y_train = np.matrix(np.eye(10)[y_train])  # vector có index = y_train thì bằng 1
y_test = np.matrix(np.eye(10)[y_test])

print("X_train", x_train.shape)  # out: (60000, 784) ==> matrix 60000 hàng (60000 ảnh), kích thước 784
print("Y_train", y_train.shape)  # out: (60000, 10)  ==> Mỗi label là 1 vector 10 phần tử one hot

# Định nghĩa hàm sigmoid
def sigmoid(x):
  return 1./(1. + np.exp(-x))

# Định nghĩa hàm softmax
def softmax(x):
  return np.divide(np.matrix(np.exp(x)), np.mat(np.sum(np.exp(x), axis=1)))

# Định nghĩa hàm ReLU
def ReLU(x):
  return np.maximum(0, x)

# Tạo hàm mô tả mạng Neuron cho việc xem kết quả
def Forwardpass(X, Wh1, bh1, Wh2, bh2, Wo, bo):
  # Hidden layer 1
  zh1 = X@Wh1.T + bh1
  ah1 = ReLU(zh1)

  # Hidden layer 2
  zh2 = ah1@Wh2.T + bh2
  ah2 = sigmoid(zh2)

  # Output Layer
  z = ah2@Wo.T + bo
  o = softmax(z)
  return o

# Tạo hàm tính Acccuracy
def AccTest(label, prediction):
  OutMaxArg = np.argmax(prediction, axis=1)
  LabelMaxArg = np.argmax(label, axis=1)
  Accuracy = np.mean(OutMaxArg == LabelMaxArg)
  return Accuracy

learningRate = 0.5
Epoch = 50   # Cho 50 epoch
NumTrainSamples = 60000   # Dùng 60000 bức ảnh để train
NumTestSamples = 10000    # Dùng 10000 bức ảnh để test

NumInputs = 784
NumHiddenUnits = 512  # 512 node ở các hidden layer
NumClasses = 10

# Bắt đầu khởi tạo các trọng số W ngẫu nhiên
# Hidden layer 1
# Tạo trọng số Wh1 ngẫu nhiên từ -0.5->0.5 gồm 1 ma trận 512 hàng và 784 cột
Wh1 = np.matrix(np.random.uniform(-0.5, 0.5, (NumHiddenUnits, NumInputs)))
# Tạo bias ngẫu nhiên từ 0 -> 0.5 gồm 1 hàng và 512 cột ứng với 512 node layer 1
bh1 = np.random.uniform(0, 0.5, (1, NumHiddenUnits))

# Hidden layer 2
# Tạo trọng số Wh2 ngẫu nhiên từ -0.5->0.5 gồm 1 ma trận 512 hàng và 512 cột
Wh2 = np.matrix(np.random.uniform(-0.5, 0.5, (NumHiddenUnits, NumHiddenUnits)))
# Tạo bias ngẫu nhiên từ 0 -> 0.5 gồm 1 hàng và 10 cột ứng với 512 node hidden layer 1
bh2 = np.random.uniform(0, 0.5, (1, NumHiddenUnits))

# Output layer
# Tạo trọng số Wo ngẫu nhiên từ -0.5->0.5 gồm 1 ma trận 10 hàng và 512 cột
Wo = np.matrix(np.random.uniform(-0.5, 0.5, (NumClasses, NumHiddenUnits)))
# Tạo bias ngẫu nhiên từ 0 -> 0.5 gồm 1 hàng và 10 cột ứng với 512 node hidden layer 1
bo = np.random.uniform(0, 0.5, (1, NumClasses))

from IPython.display import clear_output
import matplotlib.pyplot as plt

loss = []
Acc = []
Batch_size = 200  # Mỗi lần chỉ sử dụng 200 ảnh để train
# Sử dụng Mini-batch gradient descent
Stochastic_samples = np.arange(NumTrainSamples)
for ep in range(Epoch):
  np.random.shuffle(Stochastic_samples)                # Tạo index của dữ liệu ngẫu nhiên để train
  for ite in range (0, NumTrainSamples, Batch_size):   # Mỗi lần chỉ dụng 1 batch_size data để train
    Batch_samples = Stochastic_samples[ite:ite + Batch_size]
    x = x_train[Batch_samples,:]
    y = y_train[Batch_samples,:]

    # Lan truyền thẳng
    # Hidden layer 1
    zh1 = x@Wh1.T + bh1
    ah1 = ReLU(zh1)
    # Hidden layer 2
    zh2 = ah1@Wh2.T + bh2
    ah2 = sigmoid(zh2)
    # Output Layer
    z = ah2@Wo.T + bo
    o = softmax(z)

    # Tính LOSS
    loss.append(-np.sum(np.multiply(y, np.log10(o))))   # Hàm cross entropy

    # Lan truyền ngược
    # Cập nhật error cho từng hidden
    d = o - y
    dwo = np.matmul(np.transpose(d), ah2)

    dh2 = d@Wo
    dh2s = np.multiply(np.multiply(dh2, ah2), (1-ah2))
    dwh2 = np.matmul(np.transpose(dh2s), ah1)

    dh1 = dh2@Wh2
    dh1s = np.multiply(dh1, (zh1 > 0))
    dwh1 = np.matmul(np.transpose(dh1s), x)
    # Cập nhật trọng số Wo và bias
    Wo = Wo - learningRate*dwo/Batch_size

    dbo = np.mean(d)
    bo = bo - learningRate*dbo/Batch_size

    # Cập nhật trọng số wh2
    Wh2 = Wh2 - learningRate*dwh2/Batch_size

    dbh2 = np.mean(dh2)
    bh2 = bh2 - learningRate*dbh2/Batch_size

    # Cập nhật trọng số Wh1
    Wh1 = Wh1 - learningRate*dwh1/Batch_size

    dbh2 = np.mean(dh1)
    bh1 = bh1 - learningRate*dbh2/Batch_size


  # Test accuracy
  prediction = Forwardpass(x_test, Wh1, bh1, Wh2, bh2, Wo, bo)   #  cho Data test qua mô hình
  Acc_curr = AccTest(y_test, prediction)   # Tính Acc của Epoch hiện tại
  Acc.append(Acc_curr)            #  Thêm Acc vào list các epoch
  clear_output(wait=True)
  plt.plot([i for i, _ in enumerate(Acc)], Acc, 'o')
  plt.show()

  # 60000 data, batchsize = 200, tức mỗi lần train 200 data, mỗi epoch W cập nhật 300 lần
  print("Epoch:", ep)
  print("Accuracy: ", AccTest(y_test, prediction))

  print("Max Accuracy: ", np.max(Acc))

  #  Sau khi train xong, model đạt Acc cao nhất là 0.977

